{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "from collections import OrderedDict \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the files to use i.e. same number of lines\n",
    "f = open(\"126015.txt\", \"r\")\n",
    "book1 = f.read()\n",
    "book1_lines = book1.split('\\n')\n",
    "random.shuffle(book1_lines)\n",
    "book1_part1 = book1_lines[:len(book1_lines)//2]\n",
    "book1_part2 = book1_lines[len(book1_lines)//2:]\n",
    "f.close()\n",
    "\n",
    "wf = open(\"126015_1.txt\", \"w+\")\n",
    "wf.write('\\n'.join(book1_part1))\n",
    "wf.close()\n",
    "\n",
    "wf = open(\"126015_2.txt\", \"w+\")\n",
    "wf.write('\\n'.join(book1_part2))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize second file.\n",
    "f = open(\"524590.txt\", \"r\")\n",
    "book2 = f.read()\n",
    "book2_lines = book2.split('\\n')\n",
    "random.shuffle(book2_lines)\n",
    "book2_normalized = book2_lines[:len(book1_lines)//2]\n",
    "f.close()\n",
    "\n",
    "wf = open(\"524590_1.txt\", \"w+\")\n",
    "wf.write('\\n'.join(book2_normalized))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(path):\n",
    "    f = open(path, \"r\")\n",
    "    book1 = f.read()\n",
    "    \n",
    "    # Tokenize based on NLTK and regexp split criteria.\n",
    "    book1_words = word_tokenize(book1)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    book1_words = tokenizer.tokenize(\" \".join(book1_words))\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    # Remove punctuation.\n",
    "    book1_words = [x.lower() for x in book1_words if not x in string.punctuation]\n",
    "    # Remove stop words.\n",
    "    book1_words = [x for x in book1_words if not x in stop_words]\n",
    "    # Stem words.\n",
    "    ps = PorterStemmer()\n",
    "    book1_words = [ps.stem(x) for x in book1_words]\n",
    "    \n",
    "    return book1_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 126015 - Perfect Obedience A Bride’s Vow\n",
    "# Genre: Historical Romance Fiction\n",
    "book1_tokens = get_tokens(\"126015_1.txt\")\n",
    "# 126015 - Perfect Obedience A Bride’s Vow\n",
    "# Genre: Historical Romance Fiction\n",
    "book2_tokens = get_tokens(\"126015_2.txt\")\n",
    "# 524590 - 48 Hours To Die\n",
    "# Genre: Thriller Fiction\n",
    "book3_tokens = get_tokens(\"524590_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/luiszul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/luiszul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_book1_tokens = lemmatize_tokens(book1_tokens)\n",
    "lemmatized_book2_tokens = lemmatize_tokens(book2_tokens)\n",
    "lemmatized_book3_tokens = lemmatize_tokens(book3_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
